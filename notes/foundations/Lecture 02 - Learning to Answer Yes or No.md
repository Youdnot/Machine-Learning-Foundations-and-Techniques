# Lecture 02 - Learning to Answer Yes or No

上节课，我们主要简述了机器学习的定义及其重要性，并用流程图的形式介绍了机器学习的整个过程：根据模型 ${H}$，使用演算法 ${A}$，在训练样本 ${D}$ 上进行训练，得到最好的 ${h}$，其对应的 ${g}$ 就是我们最后需要的机器学习的模型函数，一般 ${g}$ 接近于目标函数 ${f}$ 。本节课将继续深入探讨机器学习问题，介绍感知机 Perceptron 模型，并推导课程的第一个机器学习算法：Perceptron Learning Algorithm（PLA）。

## Perceptron Hypothesis Set

引入这样一个例子：某银行要根据用户的年龄、性别、年收入等情况来判断是否给该用户发信用卡。现在有训练样本 ${D}$，即之前用户的信息和是否发了信用卡。这是一个典型的机器学习问题，我们要根据 ${D}$，通过 ${A}$，在 ${H}$ 中选择最好的 ${h}$，得到 ${g}$，接近目标函数 ${f}$，也就是根据先验知识建立是否给用户发信用卡的模型。银行用这个模型对以后用户进行判断：发信用卡 ${(+1)}$，不发信用卡 ${(-1)}$。

在这个机器学习的整个流程中，有一个部分非常重要：就是 `模型选择`，即 ${Hypothesis\ Set}$ 。选择什么样的模型，很大程度上会影响机器学习的效果和表现。下面介绍一个简单常用的 ${Hypothesis\ Set}$ ：感知机（Perceptron）。

还是刚才银行是否给用户发信用卡的例子，我们把用户的个人信息作为特征向量 ${x}$，令总共有 ${d}$ 个特征，每个特征赋予不同的权重 ${w}$，表示该特征对输出（是否发信用卡）的影响有多大。那所有特征的加权和的值与一个设定的阈值 ${threshold}$ 进行比较：大于这个阈值，输出为 ${(+1)}$，即发信用卡；小于这个阈值，输出为 ${(-1)}$，即不发信用卡。感知机模型，就是当特征加权和与阈值的差大于或等于 ${0}$，则输出 ${h(x) = +1}$ ；当特征加权和与阈值的差小于 ${0}$，则输出 ${h(x) = -1}$，而我们的目的就是计算出所有权值 ${w}$ 和阈值 ${threshold}$ 。

![Perceptron](http://ofqm89vhw.bkt.clouddn.com/caaa7f079e66ebaea2a1df4c7872ed8a.png)

为了计算方便，通常我们将阈值 ${threshold}$ 当做 ${w_0}$，引入一个 ${x_0 = 1}$ 的量与 ${w_0}$ 相乘，这样就把 ${threshold}$ 也转变成了权值 ${w_0}$，简化了计算。 ${h(x)}$ 的表达式做如下变换：

![Vector Form of Perceptron Hypothesis](http://ofqm89vhw.bkt.clouddn.com/c217213eb3f46406edc7b515b3d296e7.png)

为了更清晰地说明感知机模型，我们假设 Perceptrons 在二维平面上，即 ${h(x) = sign(w_0 + w_1 x_1 + w_2 x_2)}$。其中，${w_0 + w_1 x 1 + w_2 x_2=0}$ 是平面上一条分类直线，直线一侧是正类 ${(+1)}$，直线另一侧是负类 ${(-1)}$ 。权重 ${w}$ 不同，对应于平面上不同的直线。

![Perceptrons](http://ofqm89vhw.bkt.clouddn.com/fda4f4a3440a4bb21f3ad7c249b54fac.png)

那么，我们所说的 Perceptron，在这个模型上就是一条直线，称之为 linear(binary) classifiers。注意一下，感知器线性分类不限定在二维空间中，在 ${3D}$ 中，线性分类用平面表示，在更高维度中，线性分类用超平面表示，即只要是形如 ${w^{T}x}$ 的线性模型就都属于 linear(binary) classifiers。

同时，需要注意的是，这里所说的 linear(binary) classifiers 是用简单的感知器模型建立的，线性分类问题还可以使用logistic regression来解决，后面将会介绍。

## Perceptron Learning Algorithm(PLA)

根据上一部分的介绍，我们已经知道了 hypothesis set 由许多条直线构成。接下来，我们的目的就是如何设计一个演算法 ${A}$，来选择一个最好的直线，能将平面上所有的正类和负类 `完全分开`，也就是找到最好的 ${g}$，使 ${g \approx f}$ 。

如何找到这样一条最好的直线呢？我们可以使用 `逐点修正` 的思想，首先在平面上随意取一条直线，看看哪些点分类错误。然后开始对第一个错误点就行修正，即变换直线的位置，使这个错误点变成分类正确的点。接着，再对第二个、第三个等所有的错误分类点就行直线纠正，直到所有的点都完全分类正确了，就得到了最好的直线。这种“逐步修正”，就是 PLA 思想所在。

下面介绍一下 PLA 是怎么做的。首先随机选择一条直线进行分类。然后找到第一个分类错误的点，如果这个点表示正类，被误分为负类，即 ${w_t^{T} x_{n(t)} < 0}$，那表示 ${w}$ 和 ${x}$ 夹角大于 ${90}$ 度，其中 ${w}$ 是直线的法向量。所以，${x}$ 被误分在直线的下侧（相对于法向量，法向量的方向即为正类所在的一侧），修正的方法就是使 ${w}$ 和 ${x}$ 夹角小于 ${90}$ 度。通常做法是 ${w \leftarrow w + yx,\ y = 1}$，如图右上角所示，一次或多次更新后的 ${w + yx}$ 与 ${x}$ 夹角小于 ${90}$ 度，能保证 ${x}$ 位于直线的上侧，则对误分为负类的错误点完成了直线修正。

![Perceptron Learning Algorithm](http://ofqm89vhw.bkt.clouddn.com/d257197ec773a5c7732370d0ae684943.png)

同理，如果是误分为正类的点，即 ${w_t^{T} x_{n(t)} > 0}$，那表示 ${w}$ 和 ${x}$ 夹角小于 ${90}$ 度，其中 ${w}$ 是直线的法向量。所以，${x}$ 被误分在直线的上侧，修正的方法就是使 ${w}$ 和 ${x}$ 夹角大于 ${90}$ 度。通常做法是 ${w = w + yx,\ y = -1}$ ，如图所示，一次或多次更新后的 ${w + yx}$ 与 ${x}$ 夹角大于 ${90}$ 度，能保证 ${x}$ 位于直线的下侧，则对误分为正类的错误点也完成了直线修正。

按照这种思想，遇到个错误点就进行修正，不断迭代。要注意一点：每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是没关系，不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）。这种做法的思想是“知错能改”，有句话形容它：**"A fault confessed is half redressed."**

实际操作中，可以一个点一个点地遍历，发现分类错误的点就进行修正，直到所有点全部分类正确。这种被称为Cyclic PLA。

对PLA，我们需要考虑以下两个问题：

- PLA 迭代一定会停下来吗？如果线性不可分怎么办？
- PLA 停下来的时候，是否能保证 ${f \approx g}$ ？如果没有停下来，是否有 ${f \approx g}$ ？

## Guarantee of PLA

PLA 什么时候会停下来呢？根据 PLA 的定义，当找到一条直线，能将所有平面上的点都分类正确，那么 PLA 就停止了。要达到这个终止条件，就必须保证 ${D}$ 是线性可分（linear separable）。**如果是非线性可分的，那么，PLA就不会停止。**

对于线性可分的情况，如果有这样一条直线，能够将正类和负类完全分开，令这时候的目标权重为 ${w_f}$，则对每个点，必然满足 ${y_n = sign(w_f^{T} x_n)}$，即对任一点：

$${y_{n(t)} w_f^T x_{n(t)} \ \geq \ \min_{n} y_n w_f^T x_{n(t)} > 0}$$

> 因为每一个点都被正确分类，所以 ${y w^T x > 0}$ 对每一个点都恒成立。

PLA 会对每次错误的点进行修正，更新权重 ${w_{t+1}}$ 的值，如果 ${w_{t+1}}$ 与 ${w_f}$ 越来越接近，数学运算上就是内积越大，那表示 ${w_{t+1}}$ 是在接近目标权重 ${w_f}$，证明PLA是有学习效果的。所以，我们来计算 ${w_{t+1}}$ 与 ${w_f}$ 的内积：

$${w_f^T w_{t+1} = w_f^T(w_f^Tw_t + y_{n(t)} x_{n(t)}) \geq w_f^Tw_t + \min_{n} y_n w_f^T x_n > w_f^T w_t + 0}$$

从推导可以看出，${w_{t+1}}$ 与 ${w_f}$ 的内积跟 ${w_t}$ 与 ${w_f}$ 的内积相比更大了。似乎说明了 ${w_{t+1}}$ 更接近 ${w_f}$，但是内积更大，可能是向量长度更大了，不一定是向量间角度更小。所以，下一步，我们还需要证明 ${w_{t+1}}$ 与 ${w_t}$ 向量长度的关系：

$${||w_{t+1}||^2 = ||w_t + y_{n(t)}x_{n(t)}||^2 = ||w_t||^2 + 2 y_{n(t)}w_t^Tx_{n(t)} + ||y_{n(t)}x_{n(t)}||^2 \leq ||w_t||^2 + 0+ ||y_{n(t)}x_{n(t)}||^2 \leq \max_{n} ||w_t||^2 + ||y_{n}x_{n}||^2}$$

${w_t}$ 只会在分类错误的情况下更新，最终得到的 ${||w^{2}_{t+1}||^2}$ 相比 ${||w_{t}^{2}||}$ 的增量值不超过 ${max||x^2_n||}$ 。也就是说，${w_t}$ 的增长被限制了，${w_{t+1}}$ 与 ${w_t}$ 向量长度不会差别太大。

如果令初始权值 ${w_0 = 0}$，那么经过 ${T}$ 次错误修正后，有如下结论：

$${\frac{w_f^T}{||w_f||} \frac{w_T}{||w_T||} \geq \sqrt{T} \cdot constant}$$

上述不等式左边其实是 ${w^T}$与 ${w_f}$ 夹角的余弦值，随着 ${T}$ 增大，该余弦值越来越接近 ${1}$，即 ${w^T}$ 与 ${w_f}$ 越来越接近。同时，需要注意的是，${\sqrt{T} \cdot constant \leq 1}$，也就是说，迭代次数 ${T}$ 是有上界的。根据以上证明，我们最终得到的结论是：${w_{t+1}}$ 与 ${w_f}$ 的是随着迭代次数增加，逐渐接近的。而且，PLA 最终会停下来（因为 ${T}$ 有上界），实现对线性可分的数据集完全分类。

## Non-Separable Data

上一部分，我们证明了线性可分的情况下，PLA 是可以停下来并正确分类的，但对于非线性可分的情况，${w_f}$ 实际上并不存在，那么之前的推导并不成立，PLA 不一定会停下来。所以，PLA 虽然实现简单，但也有缺点：

![Pros and Cons](http://ofqm89vhw.bkt.clouddn.com/89edcd6428f62d7adbe03b86dc52efa0.png)

对于非线性可分的情况，我们可以把它当成是数据集 ${D}$ 中掺杂了一下 noise，事实上，大多数情况下我们遇到的 ${D}$，都或多或少地掺杂了 noise。这时，机器学习流程是这样的：

![Learning with Noisy Data](http://ofqm89vhw.bkt.clouddn.com/d386df1802a7dc504e844e9ba3201686.png)

在非线性情况下，我们可以把条件放松，即不苛求每个点都分类正确，而是容忍有错误点，取错误点的个数最少时的权重 ${w}$ ：

![Line with Noise Tolerance](http://ofqm89vhw.bkt.clouddn.com/ba86a35d6b2091310ca0b1af8a8933c6.png)

事实证明，上面的解是 NP-hard 问题，难以求解。然而，我们可以对在线性可分类型中表现很好的 PLA 做个修改，把它应用到非线性可分类型中，获得近似最好的 ${g}$。

修改后的 PLA 称为 Packet Algorithm。它的算法流程与 PLA 基本类似，首先初始化权重 ${w_0}$，计算出在这条初始化的直线中，分类错误点的个数。然后对错误点进行修正，更新 ${w}$，得到一条新的直线，在计算其对应的分类错误的点的个数，并与之前错误点个数比较，取个数较小的直线作为我们当前选择的分类直线。之后，再经过 ${n}$ 次迭代，不断比较当前分类错误点个数与之前最少的错误点个数比较，选择最小的值保存。直到迭代次数完成后，选取个数最少的直线对应的 ${w}$，即为我们最终想要得到的权重值。

如何判断数据集 ${D}$ 是不是线性可分？对于二维数据来说，通常还是通过肉眼观察来判断的。一般情况下， Pocket Algorithm 要比 PLA 速度慢一些。

## 总结

本节课主要介绍了线性感知机模型，以及解决这类感知机分类问题的简单算法：PLA。我们详细证明了对于线性可分问题，PLA 可以停下来并实现完全正确分类。对于不是线性可分的问题，可以使用 PLA 的修正算法 Pocket Algorithm来解决。

## 参考

1. [台湾大学林轩田机器学习基石课程学习笔记2 -- Learning to Answer Yes/No](http://blog.csdn.net/red_stone1/article/details/70866527)
1. [NP (复杂度) - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/zh-hans/NP_(複雜度))